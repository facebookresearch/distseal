#========================================================================================
# Mandatory parameters. Must be specified in command line or script
output_dir: ""  # Output to store checkpoints, intermediate results (images), and logs

# dataset
dataset:
    train_dir: /path/to/imagenet_full_size/061417/train/
    val_dir: /path/to/imagenet_full_size/061417/val/
    train_annotation_file: null
    val_annotation_file: null




#========================================================================================
# Model-specific parameters. Modify these values for different experiments
# dataloading
img_size_proc: 512
img_size_val: 512
img_size: 512

# watermarker
embedder_model: unet_small2_yuv_quant
scaling_w: 0.2
scaling_w_schedule: Cosine,scaling_min=0.02,start_epoch=100,epochs=100




#========================================================================================
# Default values + shared values across experiments
seed: 0

# dataloading
batch_size_eval: 2
batch_size: 16

# watermarker
extractor_model: convnext_tiny
nbits: 64
hidden_size_multiplier: 1
attenuation: null
scaling_i: 1.0
videowam_step_size: 8

# augmentation
num_augs: 2
augmentation_config: configs/augmentation/all_augs_v3.yaml

# loss
balanced: False
total_gnorm: 1.0
lambda_dec: 1.0
lambda_det: 0.0
lambda_d: 0.1
lambda_i: 0.0
perceptual_loss: mse
disc_num_layers: 3
maskbit_maxpool: 8
maskbit_disc: true
disc_in_channels: 3
disc_start: 200

# training
epochs: 601
iter_per_epoch: 1000
eval_freq: 10
full_eval_freq: 50
saveimg_freq: 50
iter_per_valid: 10
scheduler: CosineLRScheduler,lr_min=1e-6,t_initial=601,warmup_lr_init=1e-8,warmup_t=20
optimizer: AdamW,lr=5e-4
finetune_detector_start: 2000

# distribute
workers: 8
