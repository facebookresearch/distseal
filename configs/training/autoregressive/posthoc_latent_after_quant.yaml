
#========================================================================================
# Mandatory parameters. Must be specified in command line or script
output_dir: ""  # Output to store checkpoints, intermediate results (images), and logs

dataset:
    train_dir: /path/to/imagenet_full_size/061417/train/
    val_dir: /path/to/imagenet_full_size/061417/val/
    train_annotation_file: null
    val_annotation_file: null




#========================================================================================
# Model-specific parameters. Modify these values for different experiments
# dataloading:
img_size_proc: 256
img_size_val: 256
img_size: 256


# watermarker:
embedder_model: unet_bottleneck
autoencoder: MaskgitVqgan
latent_layer: input_after_quantize
scaling_w: 2.0
scaling_w_schedule: Cosine,scaling_min=0.5,start_epoch=100,epochs=100





#========================================================================================
# Default values + shared values across experiments
seed: 0

# dataloading:
batch_size_eval: 8
batch_size: 16

# watermarker:
extractor_model: convnext_tiny
nbits: 64
hidden_size_multiplier: 1
attenuation: null
ae_quantize: true
scaling_i: 1.0
videowam_step_size: 8

# augmentation
num_augs: 2
augmentation_config: configs/augmentation/all_augs_v3.yaml

# loss
balanced: false
total_gnorm: 1.0
lambda_dec: 1.0
lambda_det: 0.0
lambda_d: 0.1
lambda_i: 0.0
perceptual_loss: mse
maskbit_disc: true
maskbit_maxpool: 8
disc_num_layers: 3
disc_in_channels: 3
disc_start: 200

# training
epochs: 601
iter_per_epoch: 1000
eval_freq: 10
saveimg_freq: 50
iter_per_valid: 10
scheduler: CosineLRScheduler,lr_min=1e-6,t_initial=601,warmup_lr_init=1e-8,warmup_t=20
optimizer: AdamW,lr=5e-4
full_eval_freq: 50
finetune_detector_start: 2000

# distributed:
workers: 8